{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOjLithgoLfUUuqBc0ScI1S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0DTjaF-kewM",
        "colab_type": "text"
      },
      "source": [
        "*TP2 : INDEXATION D’IMAGES*\n",
        "\n",
        "*Réalisé par : ESSO Dissirama - NIKUE AMASSAH Djahlin Hervé*\n",
        "\n",
        "*P23/SIM*\n",
        "\n",
        "*Année scolaire 2019-2020*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX8idTz6MgdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "2b56ce3c-6646-4c58-e938-68bbbf9262c4"
      },
      "source": [
        "# Importation de mon drive dans colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER8xmJJ2Mjah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "56bc62f5-b107-432a-b332-a8c441bc6c5c"
      },
      "source": [
        "# Installation de opencv 3.3.0.10\n",
        "pip install opencv-python==3.3.0.10 opencv_contrib-python==3.3.0.10"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opencv-python==3.3.0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/da/d7cc32630062e75cac5df51fcb2fef0786c7037aa76eb500ee5ec6fc881a/opencv_python-3.3.0.10-cp36-cp36m-manylinux1_x86_64.whl (15.4MB)\n",
            "\u001b[K     |████████████████████████████████| 15.5MB 294kB/s \n",
            "\u001b[?25hCollecting opencv_contrib-python==3.3.0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/8c/2ceec0162a5978592e9ca9a47976d61de3c28c55d2710045ded9c2333b78/opencv_contrib_python-3.3.0.10-cp36-cp36m-manylinux1_x86_64.whl (21.4MB)\n",
            "\u001b[K     |████████████████████████████████| 21.4MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.3.0.10) (1.17.5)\n",
            "\u001b[31mERROR: dopamine-rl 1.0.5 has requirement opencv-python>=3.4.1.15, but you'll have opencv-python 3.3.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python, opencv-contrib-python\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Successfully installed opencv-contrib-python-3.3.0.10 opencv-python-3.3.0.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzO0PSZuMlap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copie de la base d'image dans colab\n",
        "cp -R /content/drive/My\\ Drive/image.orig /"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Cu1AumMd23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Définition du chemain d'accès à la base d'images\n",
        "chemain = \"/image.orig\"\n",
        "\n",
        "# Nombre de cluster pour créer le sac de mots visuels avec kmeans fonction\n",
        "n_cluster = 300\n",
        "# Tableau pour stocker les descripteurs à extraire des images\n",
        "descripteur_table = []\n",
        "\n",
        "# Dictionnaire contenant le nom des images et les descripteurs asociés à chaque image\n",
        "sift_vectors = {}\n",
        "\n",
        "# Tableau contenant les descripteur des images ordonnés par classe\n",
        "descripteur = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ulhHCQdmFod",
        "colab_type": "text"
      },
      "source": [
        "*Première partie  : Conception d’un moteur de recherche d’images simple (sans\n",
        "                            structuration des index*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmNJMVk9M2Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Fonction de création du sac de mots visuels avec kmeans avec 2 parametres :\n",
        "# K = nombre de cluster et descriptor_list = liste des descripteurs sift des images\n",
        "# La fonction retourne le modèle kmeans créé à partir des descripteurs de chaque image\n",
        "def bag_of_word_with_kmeans(k, descriptor_list):\n",
        "    # Initialisation du Kmeans\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
        "    # Entraînement du kmeans sur la liste de descripteurs pour créer le modèle kmeans\n",
        "    kmeans.fit(descriptor_list)\n",
        "\n",
        "    # Retourner le modèle kmeans formé\n",
        "    return kmeans\n",
        "\n",
        "\n",
        "# function de construction d'histogramme d'une image\n",
        "# Prend 2 paramètres : le model kmeans obtenu (sac de mot visuels) et les descripteurs correspondant à une image\n",
        "# Retourne l'histogramme normalisé d'une image\n",
        "def build_image_histogram(clustering_model, descriptors):\n",
        "    # Initialisation de la taille de l'histogramme de l'image avec le nombre de cluster\n",
        "    hist = np.zeros(n_cluster)\n",
        "    # Calcule du nombre total de descripteur pour une image afin de l'utiliser pour normaliser l'histogramme\n",
        "    n_desc = np.size(descriptors)\n",
        "    # Parcours des descripteurs d'une image afin de calculer son histogramme\n",
        "    for desc in descriptors:\n",
        "        # Récupération de l'indice du cluster auquel le descripteur est plus proche\n",
        "        idx = clustering_model.predict([desc])\n",
        "        # Construction de l'histogramme de l'image avec le cluster prédict et le pourcentage du nombre de descripteurs\n",
        "        # associé à ce cluster. On ajoute à l'indice idx (cluster plus proche) du tableau de l'histogramme (hist) le pourcentage du nombre de descripteurs\n",
        "        # proche du cluster prédict par le modèle kmeans sur chaque descripteur de l'image.\n",
        "        hist[idx] += 1/n_desc\n",
        "\n",
        "    # retourner le tableau contenant l'histogramme de l'image\n",
        "    return hist\n",
        "\n",
        "# function de construction d'histogramme de toutes les imagees\n",
        "# Prend 2 paramètres : le model kmeans obtenu (sac de mot visuels) et la liste des descripteurs ordonnée par classe\n",
        "# Retourne un tableau contenant les histogrammes de toutes les images\n",
        "def build_all_images_histogram(clustering_model, imgs_descriptors_list):\n",
        "    # Initialisation du tableau des histogrammes\n",
        "    histograms = []\n",
        "    # Parcourir la liste des descripteurs des images pour récupérer les descripteurs de chaque image\n",
        "    # pour calculer l'histogramme de chaque image. Ensuite on ajoute l'histogramme calculé dans le\n",
        "    # tableau des histogrammes.\n",
        "    for des in imgs_descriptors_list:\n",
        "        # Calcul l'histogramme d'une image\n",
        "        hist = build_image_histogram(clustering_model, des)\n",
        "        # Ajout de l'histogramme d'une image dans le tableau des histogrammes\n",
        "        histograms.append(hist)\n",
        "\n",
        "    # Retourner le tableau contenant tous les histogrammes des images\n",
        "    return histograms\n",
        "\n",
        "\n",
        "\n",
        "# Fonction indexation : Fonction pour extraire les descripteurs des images de la base et créer le sac de mots visuels\n",
        "# en utilisant kmeans.\n",
        "# La fonction retourne le modèle kmeans formé sur les images et le tableau des histogrammes des images.\n",
        "def indexation(chemain):\n",
        "    # Parcourir les images dans la base d'images pour extraire les descripteurs des images\n",
        "    for path, dirs, files in os.walk(chemain):\n",
        "\n",
        "        for file in files:\n",
        "            \n",
        "            # Récupération du chemain d'accès complet de chaque image\n",
        "            image_file = os.path.join(path, file)\n",
        "            # Lecture de l'image en cours\n",
        "            image = cv2.imread(image_file)\n",
        "            # convertir l'image en cours en image blanc noir\n",
        "            image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            # Initialisation du détecteur sift\n",
        "            sift = cv2.xfeatures2d.SIFT_create()\n",
        "            # Caclcul et Extraction des descripteurs de l'image en cours\n",
        "            keys, descriptor = sift.detectAndCompute(image_gray, None)\n",
        "            # Stockage des descripteurs extraits de l'image en cours dans un tableau non ordonné\n",
        "            descripteur_table.extend(descriptor)\n",
        "            # Création d'un dictionnaire contenant le nom des images avec la liste des descripteurs correspondants\n",
        "            sift_vectors[file] = descripteur_table\n",
        "            # Stockage des descripteurs extraits de l'image en cours dans un tableau ordonné par classe\n",
        "            descripteur.append(descriptor)\n",
        "\n",
        "    print(\"******descripteur*******\")\n",
        "    print(descripteur_table[0])\n",
        "    print(\"******image descripteur*******\")\n",
        "    print(descripteur[0])\n",
        "    # Création du sac de mots visuels en utilisant kmeans\n",
        "    clustering_model = bag_of_word_with_kmeans(n_cluster, descripteur_table)\n",
        "\n",
        "    # Création de la table des histogrammes des images\n",
        "    histogram_feature = build_all_images_histogram(clustering_model, descripteur)\n",
        "    print(len(histogram_feature))\n",
        "    # Stockage de l'histogramme dans un fichier binaire pour une utilisation ultérieur\n",
        "    pickle.dump(histogram_feature, open(\"hist_file\", 'wb'))\n",
        "    print('Fichier histogramme enregistré avec succès')\n",
        "\n",
        "    # Stockage du modèle kmeans dans un fichier binaire pour une utilisation ultérieur\n",
        "    pickle.dump(clustering_model, open(\"model_file\", 'wb'))\n",
        "    print('Modèle kmeans enregistré avec succès')\n",
        "    # retourner le modèle kmeans et le tableau des histogrammes\n",
        "    return clustering_model, histogram_feature\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqyC8yh6NBiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d498e7df-7d68-4ea5-c6bb-fb3de3e6bcbe"
      },
      "source": [
        "# Appel de la fonction indexation et récupération du modèle kmeans et du tableau des histogrammes\n",
        "kmeans, histograms = indexation(chemain)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******descripteur*******\n",
            "[ 19.   0.   0.   0.   1.   1.   1.   8. 157.   0.   0.   0.   0.   0.\n",
            "   0.  54. 157.   0.   0.   0.   0.   0.   0.  93.  68.   0.   0.   0.\n",
            "   0.   0.   0.  39.  34.   5.   1.   2.   0.   0.   0.   3. 157.   5.\n",
            "   0.   0.   0.   0.   0.  52. 157.   3.   0.   0.   0.   0.   0.  49.\n",
            " 107.   5.   0.   0.   0.   0.   0.  10.  36.   3.   0.   1.   1.   1.\n",
            "   1.   3. 157.  65.   0.   0.   0.   0.   0.   4. 157.  61.   0.   0.\n",
            "   0.   0.   0.   6.  56.  43.  10.   0.   0.   0.   0.   2.  11.   3.\n",
            "   0.   1.   0.   0.   0.   3. 157.  80.   0.   0.   0.   0.   0.   2.\n",
            " 157.  75.   1.   0.   0.   0.   2.   9.  16.  40.  15.   0.   0.   0.\n",
            "   2.   2.]\n",
            "******image descripteur*******\n",
            "[[ 19.   0.   0. ...   0.   2.   2.]\n",
            " [ 45. 149.   0. ...   3.   2.   0.]\n",
            " [ 24.  85.   5. ...   0.  28. 130.]\n",
            " ...\n",
            " [  1.   1.   0. ...  12.  13.   7.]\n",
            " [ 17.  57.  33. ...   0.   0.   0.]\n",
            " [ 15.  17.   7. ...  10.   0.   1.]]\n",
            "1000\n",
            "Fichier histogramme enregistré avec succès\n",
            "Modèle kmeans enregistré avec succès\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK7Iu1L1OTMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#histogram_feature = pickle.load( open( \"/hist_file\", \"rb\" ) )\n",
        "#kmeans = pickle.load( open( \"/model_file\", \"rb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bayFqV7jNaKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tableau contenant le nom des images\n",
        "image = []\n",
        "# récupération du nom des images associé aux descripteurs pour effectuer la prédiction avec la fonction recherche\n",
        "for k, v in sift_vectors.items():\n",
        "  image.append(k)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slcDqKwCNXrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction recherche :  permet de rechercher un nombre N d'images proches d'une image test entrée en paramètres\n",
        "# Retourne les distance des cluster trouvés ainsi que leur indice et le mapping avec les images de la base.\n",
        "\n",
        "def recherche(image_path, N):\n",
        "    #lecture de l'image\n",
        "    data = cv2.imread(image_path)\n",
        "    #convertir en blanc noir\n",
        "    data = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
        "    # extraction des descripteurs sift\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "    keypoint, descriptor = sift.detectAndCompute(data, None)\n",
        "    # calcule de l'histogramme de l'image test\n",
        "    histogram = build_image_histogram(kmeans, descriptor)\n",
        "    # Recherche des images voisins proche de l'image test en utilisant l'histogramme de toutes les images\n",
        "    # On fait un entraînement du modèle du plus proche voisin sur l'ensemble des histogrammes des images \n",
        "    neighbor = NearestNeighbors(n_neighbors = N)\n",
        "    neighbor.fit(histograms)\n",
        "    # récupération des distances à chaque image plus proche trouvé et des indices des clusters correspondants\n",
        "    dist, result = neighbor.kneighbors([histogram])\n",
        "    result = np.asarray(result)\n",
        "    # mapping avec la base d'image et des indices des cluster récupérer pour retrouver les images voisines prédict par le modèle\n",
        "    image2 = list(map(lambda img: image[int(img)], result[0]))\n",
        "\n",
        "    # Retourner les distances, les indices des cluster et les images trouvées\n",
        "    return dist, result , image2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9tm96caamFM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8424c40f-5470-43e5-eb91-87367d353104"
      },
      "source": [
        "# Chemain de l'image test\n",
        "test = \"/image.orig/39.jpg\"\n",
        "# Test sur une image pour voir comment se présente les résultats\n",
        "dist, result, imgs = recherche(test, 10)\n",
        "# Affichage des distances\n",
        "print(dist)\n",
        "# Affichage des images\n",
        "print(imgs)\n",
        "# Affichage des indices\n",
        "result"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.0004305  0.00044363 0.00044442 0.00044623 0.00045427\n",
            "  0.00045614 0.00045724 0.00045949 0.00045951]]\n",
            "['39.jpg', '23.jpg', '257.jpg', '96.jpg', '68.jpg', '64.jpg', '504.jpg', '12.jpg', '47.jpg', '41.jpg']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[540,  80, 927, 517, 609, 246, 720, 950, 836, 101]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exNbPf3uNSJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Tableau pour enrégistrer les précisions pour chaque image rechercher avec la fonction recherche\n",
        "average_for_image = []\n",
        "\n",
        "# Fonction pour calculer la précision pour chaque image\n",
        "# Prend comme paramètre le chemain de la base d'image et le nombre N de voisins le plus proche\n",
        "# Retourne un tableau contenant les précision calculées pour chaque image dans la base\n",
        "def accuracy(chemain, N):\n",
        "    # Tableau pour récupérer les précisions pour chaque image\n",
        "    precision_image = []\n",
        "\n",
        "    # Boucle pour parcourir la base d'image et rechercher les N voisins les plus proche de chaque image\n",
        "    for path, dirs, files in os.walk(chemain):\n",
        "      for file in files:\n",
        "        # Récupération du nom de l'image sans le format \".jpg\" et convertion en int\n",
        "        k = int(file[:(len(file)-4)])\n",
        "        # Chemain complet pour accéder à chaque imae\n",
        "        image_file = os.path.join(path, file)\n",
        "\n",
        "        # Appel de la fonction recherche pour retrouver les N voisins les plus proche de l'image en cours\n",
        "        dist, result, img_result = recherche(image_file, N)\n",
        "        # Tableau contenant les noms des images retrouvées sans le format \".jpg\" et convertit en int pour pouvoir\n",
        "        # bien calculer la moyenne de précision\n",
        "        tab2 = [int(img[:(len(img)-4)]) for img in img_result]\n",
        "\n",
        "        # Récupération des prédictions qui sont corrects sur les N images prédict\n",
        "        # Ici on utilise le nom des images (sans la partie \".jpg\") converti en int pour pouvoir faire\n",
        "        # la correspondance entre le concept de l'image test et le concept des images retournées. On vérifie\n",
        "        # donc la plage d'entier dans laquelle le nom de l'image se trouve et on récupère dans un tableau les nom des images\n",
        "        # prédict par le modèle et qui se trouvent dans la même plage d'entier. Ainsi on fait la taille de se tableau divisée par\n",
        "        # le nombre N de voisins prédict pour calculer la précision pour l'image test.\n",
        "        if (k <= 99):\n",
        "            predict = [i for i in tab2 if i <= 99]\n",
        "\n",
        "        elif (k <= 199):\n",
        "            predict = [i for i in tab2 if i <= 199 and i >= 100]\n",
        "\n",
        "        elif (k <= 299):\n",
        "            predict = [i for i in tab2 if i <= 299 and i >= 200]\n",
        "\n",
        "        elif (k <= 399):\n",
        "            predict = [i for i in tab2 if i <= 399 and i >= 300]\n",
        "\n",
        "        elif (k <= 499):\n",
        "            predict = [i for i in tab2 if i <= 499 and i >= 400]\n",
        "\n",
        "        elif (k <= 599):\n",
        "            predict = [i for i in tab2 if i <= 599 and i >= 500]\n",
        "\n",
        "        elif (k <= 699):\n",
        "            predict = [i for i in tab2 if i <= 699 and i >= 600]\n",
        "\n",
        "        elif (k <= 799):\n",
        "            predict = [i for i in tab2 if i <= 799 and i >= 700]\n",
        "\n",
        "        elif (k <= 899):\n",
        "            predict = [i for i in tab2 if i <= 899 and i >= 800]\n",
        "\n",
        "\n",
        "        elif (k <= 999):\n",
        "            predict = [i for i in tab2 if i <= 999 and i >= 900]\n",
        "\n",
        "        # Calcule de la précision pour l'image en cours\n",
        "        average = len(predict)/N\n",
        "        # Ajout de la précision calculée pour l'image en cours dans le tableau des précisions\n",
        "        precision_image.append(average)\n",
        "\n",
        "\n",
        "    return precision_image\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFLxg1jTNizW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e2a8857-3588-4687-a0bc-03d769ec47c2"
      },
      "source": [
        "# Appel de la fonction calculer précision\n",
        "precision = accuracy(chemain, 10)\n",
        "precision_somme = 0\n",
        "# Calcule de la précision moyenne du modèle\n",
        "for i in precision:\n",
        "    precision_somme += i\n",
        "\n",
        "precision_moyenne = precision_somme/len(precision)\n",
        "\n",
        "# Affichage de la précision moyenne\n",
        "print(precision_moyenne)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47290000000000104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UlTP-hljAO-",
        "colab_type": "text"
      },
      "source": [
        "Analyse des résultats Partie 1: \n",
        "  - Après avoir calculer la précision moyenne du modèle nous avons obtenu une précision de 48%\n",
        "  - Par si on prend les résultats par concept, nous remarquons que certains concepts ont\n",
        "  de bonnes précision des résutats retournés. Par exemple la classe des chevaux (90%), 9/10\n",
        "  images des chevaux sont retournées pendant la recherche.\n",
        "  la classe des dinausors aussi (90%), 9/10 images des dinausors sont retournées pendant la recherche.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-hIxyDJl4kT",
        "colab_type": "text"
      },
      "source": [
        "*Deuxième partie  : Conception d’un moteur de recherche d’images avec structuration des index*\n",
        "                          \n",
        "*Méthode utilisée : fréquence inversé tf-idf*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCORugPqiYfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Importation de la méthode TfidfTransformer de sklearn\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Fonction pour calculer le tfidf des images à partir de leur histogramme. Cette fonction\n",
        "# prends en paramètre les histogrammes des images. Ainsi on aura une structure \n",
        "# dans laquelle à chaque mot visuel de l'histogramme va correspondre\n",
        "# une liste d'images dans laquelle ce mot viuel apparait ainsi que sa fréquence\n",
        "# d'apparition.\n",
        "def tfidf(bow):\n",
        "\t# td-idf initialisation\n",
        "  transformer = TfidfTransformer(smooth_idf=True)\n",
        "  t = transformer.fit_transform(bow).toarray()\n",
        "        \n",
        "  # normaliser par la norme euclidienne (L2) avant de retourner le modèle tf-idf \n",
        "  t = normalize(t, norm='l2', axis=1)\n",
        "    \n",
        "  return t\n",
        "\n",
        "# Appel de la méthode tfidf sur la table des histogrammes des images\n",
        "histograms = np.vstack(histograms)\n",
        "t = tfidf(histograms)\n",
        "\n",
        "# Enrégistrement du tfidf de toutes les images avec pickle\n",
        "pickle.dump(t, open(\"tfidf.pkl\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv3XGF2KHK6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction de recherche d'une image à partir de la fréquence d'apparution de ses\n",
        "# mots visuels. On récupère les mots visuels de l'image et on recherche les images de la base\n",
        "# contenant l'un des mots visuels de l'image test.\n",
        "def recherche_with_tfidf(image_path, N):\n",
        "    #lecture de l'image\n",
        "    data = cv2.imread(image_path)\n",
        "    #convertir en blanc noir\n",
        "    data = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
        "    # extraction des descripteurs sift\n",
        "    sift = cv2.xfeatures2d.SIFT_create()\n",
        "    keypoint, descriptor = sift.detectAndCompute(data, None)\n",
        "    # calcule de l'histogramme de l'image test\n",
        "    histogram = build_image_histogram(kmeans, descriptor)\n",
        "    # calcule du tfidf à partir l'histogramme de l'image test\n",
        "    image_test = tfidf(np.vstack(histogram))\n",
        "    image_test_flatten = np.array(image_test).flatten()\n",
        "    # Récupération du tfidf de toutes les images\n",
        "    allimages_tfidf = t\n",
        "    # Recherche des images voisins proche de l'image test en utilisant le tfidf de toutes les images\n",
        "    neighbor = NearestNeighbors(n_neighbors = N)\n",
        "    neighbor.fit(allimages_tfidf)\n",
        "    # récupération des distances à chaque image plus proche trouvé\n",
        "    dist, result = neighbor.kneighbors([image_test_flatten])\n",
        "    result = np.asarray(result)\n",
        "    # mapping avec la base d'image et des indices des cluster récupérer pour retrouver les images voisines prédict par le modèle\n",
        "    image2 = list(map(lambda img: image[int(img)], result[0]))\n",
        "\n",
        "    # Retourner les images trouvées\n",
        "    return image2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyf4Akt5uB_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tableau pour enrégistrer les précisions pour chaque image rechercher avec la fonction recherche\n",
        "average_for_image = []\n",
        "\n",
        "# Fonction pour calculer la précision pour chaque image\n",
        "# Prend comme paramètre le chemain de la base d'image et le nombre N de voisins le plus proche\n",
        "# Retourne un tableau contenant les précision calculées pour chaque image dans la base\n",
        "def accuracy2(chemain, N):\n",
        "    # Tableau pour récupérer les précisions pour chaque image\n",
        "    precision_image = []\n",
        "\n",
        "    # Boucle pour parcourir la base d'image et rechercher les N voisins les plus proche de chaque image\n",
        "    for path, dirs, files in os.walk(chemain):\n",
        "      for file in files:\n",
        "        # Récupération du nom de l'image sans le format \".jpg\" et convertion en int\n",
        "        k = int(file[:(len(file)-4)])\n",
        "        # Chemain complet pour accéder à chaque imae\n",
        "        image_file = os.path.join(path, file)\n",
        "\n",
        "        # Appel de la fonction recherche pour retrouver les N voisins les plus proche de l'image en cours\n",
        "        img_result = recherche_with_tfidf(image_file, N)\n",
        "        # Tableau contenant les noms des images retrouvées sans le format \".jpg\" et convertit en int pour pouvoir\n",
        "        # bien calculer la moyenne de précision\n",
        "        tab2 = [int(img[:(len(img)-4)]) for img in img_result]\n",
        "\n",
        "        # Récupération des prédictions qui sont corrects sur les N images prédict\n",
        "        # Ici on utilise le nom des images (sans la partie \".jpg\") converti en int pour pouvoir faire\n",
        "        # la correspondance entre le concept de l'image test et le concept des images retournées. On vérifie\n",
        "        # donc la plage d'entier dans laquelle le nom de l'image se trouve et on récupère dans un tableau les nom des images\n",
        "        # prédict par le modèle et qui se trouvent dans la même plage d'entier. Ainsi on fait la taille de se tableau divisée par\n",
        "        # le nombre N de voisins prédict pour calculer la précision pour l'image test.\n",
        "        if (k <= 99):\n",
        "            predict = [i for i in tab2 if i <= 99]\n",
        "\n",
        "        elif (k <= 199):\n",
        "            predict = [i for i in tab2 if i <= 199 and i >= 100]\n",
        "\n",
        "        elif (k <= 299):\n",
        "            predict = [i for i in tab2 if i <= 299 and i >= 200]\n",
        "\n",
        "        elif (k <= 399):\n",
        "            predict = [i for i in tab2 if i <= 399 and i >= 300]\n",
        "\n",
        "        elif (k <= 499):\n",
        "            predict = [i for i in tab2 if i <= 499 and i >= 400]\n",
        "\n",
        "        elif (k <= 599):\n",
        "            predict = [i for i in tab2 if i <= 599 and i >= 500]\n",
        "\n",
        "        elif (k <= 699):\n",
        "            predict = [i for i in tab2 if i <= 699 and i >= 600]\n",
        "\n",
        "        elif (k <= 799):\n",
        "            predict = [i for i in tab2 if i <= 799 and i >= 700]\n",
        "\n",
        "        elif (k <= 899):\n",
        "            predict = [i for i in tab2 if i <= 899 and i >= 800]\n",
        "\n",
        "\n",
        "        elif (k <= 999):\n",
        "            predict = [i for i in tab2 if i <= 999 and i >= 900]\n",
        "\n",
        "        # Calcule de la précision pour l'image en cours\n",
        "        average = len(predict)/N\n",
        "        # Ajout de la précision calculée pour l'image en cours dans le tableau des précisions\n",
        "        precision_image.append(average)\n",
        "\n",
        "\n",
        "    return precision_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "561YwWnUusja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14c563d0-bfc9-4420-adf2-5731202a040b"
      },
      "source": [
        "# Appel de la fonction calculer précision\n",
        "precision = accuracy2(chemain, 10)\n",
        "precision_somme = 0\n",
        "# Calcule de la précision moyenne du modèle\n",
        "for i in precision:\n",
        "    precision_somme += i\n",
        "\n",
        "precision_moyenne = precision_somme/len(precision)\n",
        "\n",
        "# Affichage de la précision moyenne\n",
        "print(precision_moyenne)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.18049999999999894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljnfjtgli0SZ",
        "colab_type": "text"
      },
      "source": [
        "Analyse des résultats Partie 2 : \n",
        "  - Après avoir calculer la précision moyenne du modèle en utilisant le tf-idf nous avons obtenu une précision myenne de 18%. Il y a donc une baisse de la précision par rapport à la première méthode. Cette baisse peut s'expliquer par le fait que on fait la recherche par rapport à la présence d'un des mots visuels de l'image recherchée dans la base d'images. Le résultat retourné peut donc contenir un mot visuel de l'image recherchée mais ne va pas correspondre réellement au même concept d'image.\n",
        "  - Nous remarquons aussi que la recherche est beaucoup plus rapide que par rapport à la première partie\n",
        "  "
      ]
    }
  ]
}